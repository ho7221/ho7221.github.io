---
title: "[DL] Lec 1. Image Classification"
category: COSE474
excerpt: "Lecture 1. Image Classification"
toc: true
toc_sticky: true
toc_label: "Table of Contents"
toc_icon: "bars"
---

# non-parametric Classifier
parameter가 필요없이 K만 있으면 되기때문에 non-parametric하다고 한다. 이때 K는 hyperparameter라고 부른다.

## K-Nearest Neighbor
Dataset에서 원하는 data point를 정의하기 위해 근접하는 K개의 점의 분류로 해당 data point를 정의하는 것.  

# parametric Classifier
선형회귀를 예로 y=Ax+b 들때 A와 b라는 parameter(가중치)가 필요하다. 이런 classifier를 parametric classifier라고 한다.
- Neural Networks
- Naive Bayes
...

# 그 중간 어딘가...
- SVM

# Linear Classifier
32x32x3 크기의 image를 10개의 class로 분류하는 예시를 들어보자. 
f(x;W,b) 에서 x를 image, W를 parameter(가중치)로 input을 받는 classifier f를 정의한다.
이때 f(x;W,b)=Wx+b에서 
- input: [3072x1] vector
- output: [10x1] vector
- Weight: [10x3072] vector 
- Bias: [10x1] vector
이며 parameter는 W,b가 된다.

관점에 따라 세가지로 나누는데
- Algebric Viewpoint: f(x,W)=Wx

- Visual Viewpoint
- Geometric Viewpoint: image들의 공간을 나누는 hyperplane.

# Bias-Variance Trade-off
## Bias
예상되는 오류의 평균

## Variance
model의 flexibility

## Bias와 Variance의 관계
### parameter가 많을때
model의 overfit이 올 수 있고 bias는 작고 variance가 크다.
### parameter가 적을떄 
model의 underfit이 올 수 있고 bias는 크고 variance가 작다.

## Variance를 줄이려면?
- classifier를 더 간단한 것으로
- parameter를 일반화해서 줄이거나
- training data를 더 얻으면 된다.

