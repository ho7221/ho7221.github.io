---
title: "[ML] Lec 2. Supervised Learning"
category: COSE362
excerpt: "Lecture 2. Supervised Learning"
toc: true
toc_sticky: true
toc_label: "Table of Contents"
toc_icon: "bars"
---
# Supervised Class Learning
Supervised Learning은 주어진 sample data로부터 특정 class를 학습하는것. 
예시로 두 parameter에 대한 학습일때 특정 class를 하나의 사각형으로 생각할 수 있다.  

## Hypothesis Class($H$)
$h(x) = 1$ if h classifies x as positivie  
$h(x) = 0$ if h classifies x as negative  
Error of $h \in H$ on $X$:  
$E(h|X)=\sum1(h(x^t)\ne r^t)$

## Version Space
$S$ : most specific hypothesis, 가장 안전한 가정  
$G$ : most general hypothesis, 가장 관대한 가정  
$S$와 $G$ 사이의 모든 $h \in H$ 는 consistent하고 version space를 만들어낸다.  

## Margin
$h$의 $S$와 $G$ 사이에서의 간격. margin이 가장 큰 hypothesis를 선택하는 것이 좋다.

## VC(Vapnik-Chervonenkis) Dimension
N개의 point를 labeling하는 경우의 수는 $2^N$이다.  
$h \in H$가 모든 point에 대해 consistent하면 $H$가 $N$을 shatter(나눈다)고 할 수 있다.  
> VC($H$)=$N$
