---
title: "[ML] Lec 3. Bayesian Decision Theory"
category: COSE362
excerpt: "Lecture 3. Bayesian Decision Theory"
toc: true
toc_sticky: true
toc_label: "Table of Contents"
toc_icon: "bars"
---
# Bayes' rule
$P(C|x)=\frac{P(x|C)P(C)}{P(x)}$  
- $P(C|x)$: posterior(사후확률)  
- $P(x|C)$: likelihood(가능성)  
- $P(C)$: prior(사전확률)  
- $P(x)$: evidence  

## Loss & Risk
Action $\alpha_i$를 행했을때 상태가 $C_k$일때 Loss를 $\lambda_{ik}$라 한다.  
Expected Risk $R(\alpha_i|x)=\sum_{k=1}^K\lambda_{ik}P(C_k|x)$ 이며 행동을 정할 때는 $R(\alpha_i|x)=\min_k R(\alpha_k|x)$ 인 $\alpha_i$ 로 정한다.  

![Posterior graph](https://github.com/ho7221/ho7221.github.io/blob/feb81ded219f0924a5bff0599bc05c63762caee0/_posts/images/ML3_posterior_loss_reject.jpg?raw=true)  
(a) Equal loss: c1과 c2의 Loss가 같아 어느쪽도 선택할 확률이 같다.  
(b) Loss of c1 > Loss of c2: c1의 loss가 더 크기때문에 c2를 선택하는 구간이 많아진다. 다르게 해석하면 c2를 선택하는데는 낮은 Posterior로도 충분하다.  
(c) with reject: 선택하기 애매한 구간에서 reject하기 때문에 선택하기 위해서 높은 posterior가 필요하다.  

# Discriminant function
Classification을 함수로 구현한 것.  
$g_i(x) = \max_kg_k(x)$ 일때 $C_i$를 선택하며 $g_i(x)$로는 다음 함수를 사용할 수 있다.  
> $-R(\alpha_i|x)$  
> $P(C_i|x)$  
> $P(x|C_i)P(C_i)$  

## K=2 Class(Dichotomizer)
$g(x) = g_1(x)-g_2(x)$일때 $g(x)\lt0$

# Association Rule
$X \to Y$ 는 관계를 의미할 뿐 인과관계를 의미하지 않는다. 예시로 구매자와 X,Y라는 상품을 가정해보자.
1. Support: $P(X,Y) = \frac{\# Customers\: who\: bought\: X\: and\: Y}{\# Customers}$  
2. Confidence: $P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{\# Customers\: who\: bought\: X\: and\: Y}{\# Customers\: who\: bought\: X}$  
3. Lift: $\frac{P(X,Y)}{P(X)P(Y)}=\frac{P(Y|X)}{P(Y)}$  

## Apriori algorithm
변수가 많으면 모든 변수에 대해 association을 고려할 수 없기 때문에 subset부터 고려하고 superset을 고려하면 경우의 수가 줄어든다. 