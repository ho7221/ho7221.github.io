---
title: "[AI]2. Regression Model, Loss Function, Overfitting"
category: COSE361
excerpt: "2022-03-07 2nd class"
toc: true
toc_sticky: true
toc_label: "Table of Contents"
toc_icon: "bars"
---
## Programming vs ML
![Programming vs ML](https://user-images.githubusercontent.com/45323902/157029598-03680f36-f2da-42fd-a06a-226cfcaf99d5.png){: .align-center}
programming은 우리가 알듯이 input에서 output으로 program을 통해 매핑되지만 ML에서는 output도 input으로 받고 program을 output으로 준다.  

## Regression(supervised learning)
개별적인 데이터에서 연속적인 함수를 도출하는것을 regression(회귀)라고 한다. $x$와 $y$ 모두 연속적인 범위에서 존재한다.  
정확한 regression을 위해서 data가 적으면 simple model로 가야한다.  
### Simple Linear Regression
$y=ax+b$
### Multiple Linear Regression
$y=a^T x + b$ (hyper plane)
> hyperplane(초평면): 한 차원을 낮춘 공간. 3차원의 경우 면, 2차원은 선과 같이 이루어진다. 
> classification에 적합한데 그 이유는 어떤 차원이든 둘로 나누기 때문이다.
### General linear Regression
$y=Ax+b$ or $Y=XA$
이때 A는 matrix이다. 여러 input, output이 존재할때 사용한다. 즉 여러개(general)의 Simple Linear Regression을 나타낸다고 생각하면 된다.

## Loss Function(Objective function)
### Estimation(Training)
실제값을 $y_i$ 라고 할 때, 추정값을 $\hat{y_i}$ 로 나타내며 이 기호는 hat이라 부른다. 
Loss function은 실제값과 추론값의 차이를 계산하는 함수로 사용한다.   
### Loss functions
1. $\displaystyle\sum_{i}{y_i-\hat{y_i}}$  
이 함수는 오류의 크기뿐 아니라 부호까지 있기 때문에 오류가 -2, +2일때 오류가 0이 되므로 Loss Function으로 사용할 수 없다.  
2. $\displaystyle\sum_{i}{\vert y_i-\hat{y_i} \vert}$  
3. $\displaystyle\sum_{i}{\vert y_i-\hat{y_i} \vert^2}$  
두 함수는 오류가 (-2,+2)일 때와 (0,-4)일 때 알 수 있는데 제곱을 함으로 오류의 가중치가 더 높아지기 때문에 세번째 함수가 더 정확하다고 할 수 있다.  

## Optimization(Minimizing cost)
### Gradient Descent Algorithm
gradient는 1차 미분방정식이란다...  
optimal solution 즉 minimum이 되는 값을 찾는 것인데 $\displaystyle\sum_{i}{\vert y_i-{w_1}x-w_0\vert^2}$를 최소로 하는 $w_0$, $w_1$을 찾는것이다.  
이 때 $\displaystyle\sum_{i}{\vert y_i-{w_1}x-w_0\vert^2}$를 model parameter라고 하며 loss function, 즉 차이를 제곱하는 요소이다. 또한 alpha는 step size(learning rate)이다.  

## Overfitting(과적합)
데이터가 적거나 모델이 복잡하면 overfitting이 발생한다. 학습이 과하면 모든 데이터에 정확히 일치하면서 그래프가 부정확해지기 때문에 적당한 step size를 정하는 것이 좋다.  
### Ockham’s razor
데이터와 상응하는 가장 간단한 hypothesis를 사용하는 것이 좋다는 이론.
